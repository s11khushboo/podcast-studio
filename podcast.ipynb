{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOk7w5KTSJ3YuzluzgKgrx8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s11khushboo/podcast-studio/blob/main/podcast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kx3WBr9OZbaJ",
        "outputId": "a5ad1a84-8b8f-4bca-f593-ad702e7e692c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.16.0)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.13.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.123.10)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.3.7)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (26.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.22)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.14)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.50.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.21.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.40.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2026.1.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (0.21.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai gradio PyPDF2 requests python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import uuid\n",
        "import requests\n",
        "import PyPDF2\n",
        "import gradio as gr\n",
        "\n",
        "from openai import OpenAI\n",
        "\n",
        "# Set your API key here or use environment variable\n",
        "OPENAI_API_KEY = \"OPENAI_API_KEY\"\n",
        "\n",
        "MODEL_NAME = \"gpt-3.5-turbo\"\n",
        "TTS_MODEL = \"gpt-4o-mini-tts\"\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "os.makedirs(\"outputs/audio\", exist_ok=True)\n",
        "os.makedirs(\"outputs/scripts\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "BncLsZY0Zh6t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_text_input(text: str) -> str:\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def load_pdf(file_obj) -> str:\n",
        "    text = \"\"\n",
        "    reader = PyPDF2.PdfReader(file_obj)\n",
        "    for page in reader.pages:\n",
        "        extracted = page.extract_text()\n",
        "        if extracted:\n",
        "            text += extracted + \"\\n\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def load_url(url: str) -> str:\n",
        "    r = requests.get(url, timeout=10)\n",
        "    return r.text[:15000]  # safety truncate\n"
      ],
      "metadata": {
        "id": "rSlZyApdZqtw"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT = \"\"\"\n",
        "You are a professional podcast script writer.\n",
        "\n",
        "Turn the provided content into an engaging spoken podcast episode.\n",
        "Style rules:\n",
        "- Conversational\n",
        "- Clear structure\n",
        "- Friendly tone\n",
        "- Include intro, main insights, and closing summary\n",
        "- Avoid bullet points ‚Äî use natural speech\n",
        "\"\"\"\n",
        "\n",
        "def transform_to_podcast_script(raw_text: str) -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_NAME,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": raw_text}\n",
        "        ],\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "uad084MVZtN3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_script(script: str) -> str:\n",
        "    intro = \"Welcome to today‚Äôs AI-generated podcast episode.\\n\\n\"\n",
        "    outro = \"\\n\\nThat wraps up today‚Äôs episode ‚Äî thanks for listening.\"\n",
        "    return intro + script + outro\n"
      ],
      "metadata": {
        "id": "bUVOqexIZwQ9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_audio(script: str) -> str:\n",
        "    file_path = f\"outputs/audio/{uuid.uuid4()}.mp3\"\n",
        "\n",
        "    with client.audio.speech.with_streaming_response.create(\n",
        "        model=TTS_MODEL,\n",
        "        voice=\"alloy\",\n",
        "        input=script\n",
        "    ) as response:\n",
        "        response.stream_to_file(file_path)\n",
        "\n",
        "    return file_path\n"
      ],
      "metadata": {
        "id": "TmmnM9rAZy8f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline_from_text(text: str):\n",
        "    raw = load_text_input(text)\n",
        "    script = transform_to_podcast_script(raw)\n",
        "    formatted = format_script(script)\n",
        "\n",
        "    script_file = f\"outputs/scripts/{uuid.uuid4()}.txt\"\n",
        "    with open(script_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(formatted)\n",
        "\n",
        "    audio_path = generate_audio(formatted)\n",
        "\n",
        "    return formatted, audio_path\n",
        "\n",
        "\n",
        "def run_pipeline_from_pdf(file_obj):\n",
        "    raw = load_pdf(file_obj)\n",
        "    return run_pipeline_from_text(raw)\n",
        "\n",
        "\n",
        "def run_pipeline_from_url(url: str):\n",
        "    raw = load_url(url)\n",
        "    return run_pipeline_from_text(raw)\n"
      ],
      "metadata": {
        "id": "UKOd1gE4Z1wc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"\n",
        "Large Language Models are transforming how people interact with technology.\n",
        "They can summarize documents, generate code, answer questions, and create media.\n",
        "\"\"\"\n",
        "\n",
        "script, audio = run_pipeline_from_text(sample_text)\n",
        "\n",
        "print(script)\n",
        "audio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585
        },
        "id": "vMMDrBbqZ4Qb",
        "outputId": "3fa61f1a-2324-44b4-d77a-167ccbfd3383"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to today‚Äôs AI-generated podcast episode.\n",
            "\n",
            "[Intro music fades out]\n",
            "\n",
            "Welcome back to [Podcast Name]! I‚Äôm your host, [Your Name], and today we‚Äôre diving into a topic that‚Äôs been buzzing around the tech world: Large Language Models, or LLMs for short. If you‚Äôve ever wondered how this fascinating technology is shifting the way we interact with our devices and the digital landscape, you‚Äôre in the right place!\n",
            "\n",
            "So, let‚Äôs get right into it. Large Language Models are more than just impressive bits of code; they‚Äôre revolutionizing the way we communicate with technology. Imagine a world where you can simply ask your computer to summarize a lengthy document, generate lines of code, or even create unique pieces of media‚Äîall with just a few words. It‚Äôs pretty incredible, isn‚Äôt it?\n",
            "\n",
            "Now, let‚Äôs break this down a bit. First off, LLMs are trained on vast amounts of text data. This training allows them to understand and generate human-like text. So when you type in a question or a request, these models analyze the context and produce a response that not only makes sense but often feels quite natural. It‚Äôs like having a conversation with a super-smart friend who knows a little bit about everything!\n",
            "\n",
            "One of the most exciting applications of LLMs is document summarization. Think about those lengthy reports or articles that you just don‚Äôt have the time to read fully. With a simple command, an LLM can distill that information into a concise summary, giving you the key points without all the fluff. This is a game changer for professionals who need to stay informed but are strapped for time.\n",
            "\n",
            "And then there‚Äôs code generation. For software developers, this can feel like magic. LLMs can help write code snippets based on the description of what you want to achieve. Instead of sifting through endless documentation or forums, you can get tailored code recommendations right when you need them. It‚Äôs not just a time-saver; it‚Äôs an innovation that can boost creativity and teamwork in tech projects.\n",
            "\n",
            "Let‚Äôs not forget about answering questions. Whether you‚Äôre looking for a quick fact or need in-depth information on a complex topic, LLMs can pull from a vast pool of knowledge to provide relevant answers. This means that whether you‚Äôre a student, a researcher, or just someone curious about the world, you have a powerful tool at your fingertips.\n",
            "\n",
            "And last but certainly not least, we have media creation. With LLMs, generating text for articles, stories, or even scripts has never been easier. It allows writers and creators to brainstorm ideas or overcome writer‚Äôs block, resulting in fresh and engaging content.\n",
            "\n",
            "[Transition music]\n",
            "\n",
            "As we wrap up today‚Äôs episode, it‚Äôs clear that Large Language Models are not just a tech trend; they‚Äôre a transformative force in how we interact with technology. From summarizing documents to generating code and creating media, these models are changing the game in ways we‚Äôre only beginning to understand.\n",
            "\n",
            "So, what does this mean for you? If you‚Äôre not already exploring how LLMs can enhance your work or daily life, now might be the perfect time to start. Embracing this technology could not only make tasks easier but also open up new avenues for creativity and productivity.\n",
            "\n",
            "Thank you for joining me today! If you enjoyed this episode, consider sharing it with a friend or leaving us a review. I‚Äôd love to hear your thoughts on Large Language Models and how you think they might shape our future. Until next time, keep exploring, and stay curious!\n",
            "\n",
            "[Outro music fades in]\n",
            "\n",
            "That wraps up today‚Äôs episode ‚Äî thanks for listening.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'outputs/audio/8fd547af-40fd-4e90-8045-96cb9a107f82.mp3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradio_text_flow(text):\n",
        "    return run_pipeline_from_text(text)\n",
        "\n",
        "\n",
        "def gradio_pdf_flow(pdf_file):\n",
        "    return run_pipeline_from_pdf(pdf_file)\n",
        "\n",
        "\n",
        "def gradio_url_flow(url):\n",
        "    return run_pipeline_from_url(url)\n",
        "\n",
        "\n",
        "with gr.Blocks(title=\"Podcast Studio\") as demo:\n",
        "    gr.Markdown(\"# üéôÔ∏è Podcast Studio ‚Äî AI Podcast Generator\")\n",
        "\n",
        "    with gr.Tab(\"Text Input\"):\n",
        "        text_in = gr.Textbox(lines=12, label=\"Paste Content\")\n",
        "        text_btn = gr.Button(\"Generate Podcast\")\n",
        "        text_script = gr.Textbox(label=\"Podcast Script\")\n",
        "        text_audio = gr.Audio(label=\"Audio Output\")\n",
        "\n",
        "        text_btn.click(gradio_text_flow, text_in, [text_script, text_audio])\n",
        "\n",
        "    with gr.Tab(\"PDF Upload\"):\n",
        "        pdf_in = gr.File(label=\"Upload PDF\")\n",
        "        pdf_btn = gr.Button(\"Generate Podcast\")\n",
        "        pdf_script = gr.Textbox(label=\"Podcast Script\")\n",
        "        pdf_audio = gr.Audio(label=\"Audio Output\")\n",
        "\n",
        "        pdf_btn.click(gradio_pdf_flow, pdf_in, [pdf_script, pdf_audio])\n",
        "\n",
        "    with gr.Tab(\"URL Article\"):\n",
        "        url_in = gr.Textbox(label=\"Article URL\")\n",
        "        url_btn = gr.Button(\"Generate Podcast\")\n",
        "        url_script = gr.Textbox(label=\"Podcast Script\")\n",
        "        url_audio = gr.Audio(label=\"Audio Output\")\n",
        "\n",
        "        url_btn.click(gradio_url_flow, url_in, [url_script, url_audio])\n",
        "\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "xUNSvCeja9dO",
        "outputId": "9b7e8918-d2b5-42cd-8b1c-4b568e887241"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d698015c406dfb1aed.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d698015c406dfb1aed.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}